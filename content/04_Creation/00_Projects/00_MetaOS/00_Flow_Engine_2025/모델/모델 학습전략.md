
## 무거운 파인튜닝은 (학습) GPU,최고성능 요구 
하지만 추론 , Merge,GGUF 변환은 ~~결과물이 가벼워~~ 가능
-> 학습한 결과물은 용량이 가볍고, 구조가 단순하기 때문

## 학습은 코랩 (로컬 -> 코랩 파일에 저장(업로드))
- PC방 이용 (무거운 데이터 대역폭을 사용할때)
- 추론,병합,GGUF 변환은 로컬에서 가능
- 구글 드라이브에 로컬모델 넣고 -> 코랩LoRA학습

## 전략
1. HF Model -> SSD
2. 가까운 PC방, 장소, Google Drive Upload
3. Colab에서 Drive Mount -> LoRA 학습
4. 학습된 adapter를 다시 Drive or Local 저장


## checkpoint-1000에 들어있는것
| 파일명                            | 설명                                          |
| ------------------------------ | ------------------------------------------- |
| `adapter_model.bin`            | LoRA adapter의 실제 weight 값                   |
| `adapter_config.json`          | adapter 구조와 설정 (r, alpha, target_modules 등) |
| `training_args.bin`            | Trainer 설정값 저장                              |
| `optimizer.pt`, `scheduler.pt` | 옵티마이저 상태 (필요 시 resume용)                     |
| `pytorch_model.bin`            | 일반적으로 base 모델 weight지만, LoRA만 저장될 수도 있음     |
| `config.json`                  | Huggingface 모델 설정                           |

## Merge
LoRA 구조에서는 모델이 두 부분으로 나뉨:

1. **Base model (예: mistralai/Mistral-7B-Instruct-v0.2)**
2. **LoRA adapter (학습된 변화 부분만)**

> `merge`는 이 두 개를 **하나로 합치는 것**  
> 즉, LoRA 구조 없이 일반 모델처럼 `.generate()` 등에서 사용할 수 있게 만드는 것.

>`merge_and_unload()`은 LoRA adapter를 base 모델에 병합시키고, LoRA 의존성을 제거한 순수 모델로 만듦.

**완전한 자율학습 =  
강화학습(RL) + 자기지도(SSL) + 검색(RAG) + 창의적 리프레임 + 인간 감정 협력**  
→ **이 전체를 조율하는 메타 레이어가 바로 메타OS다.**